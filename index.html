<!doctype html>
<html lang="en" class="no-js">

<head>
    <meta charset="utf-8"> <!-- begin SEO -->
    <title> Canyu Chen’s Homepage</title>
    <meta property="og:locale" content="en-US">
    <meta property="og:site_name" content="Canyu Chen's Homepage">
    <meta property="og:title" content="Canyu Chen’s Homepage">
    <link rel="canonical" href="https://canyuchen.com/">
    <meta property="og:url" content="https://canyuchen.com/">
    <meta property="og:description" content="About me">
    <script
        type="application/ld+json"> { "@context" : "http://schema.org", "@type" : "Person", "name" : "Canyu Chen", "url" : "https://canyuchen.com", "sameAs" : null } </script>
    <!-- end SEO -->
    <link type="application/atom+xml" rel="alternate" title="Canyu Chen's Homepage Feed"> <!-- http://t.co/dKP3o1e -->
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script> document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js '; </script>
    <!-- For all browsers -->
    <link rel="stylesheet" href="assets/css/main.css">
    <meta http-equiv="cleartype" content="on"> <!-- start custom head snippets -->
    <link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
    <meta name="msapplication-TileColor" content="#000000">
    <meta name="theme-color" content="#ffffff">
    <script
        type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
    <script
        type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } }); </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>
    <!-- end custom head snippets -->
    <style>
        a {
            text-decoration: none !important;
        }
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
</head>

<body>

    <!--[if lt IE 9]><div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->
    <div class="masthead">
        <div class="masthead__inner-wrap">
            <div class="masthead__menu">
                <nav id="site-nav" class="greedy-nav"> <button>
                        <div class="navicon"></div>
                    </button>
                    <ul class="visible-links">
                        <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://canyuchen.com/">Canyu
                                Chen's Homepage</a></li>
                    </ul>
                    <ul class="hidden-links hidden"></ul>
                </nav>
            </div>
        </div>
    </div>
    <div id="main" role="main">
        <div class="sidebar sticky">
            <div itemscope itemtype="http://schema.org/Person">
                <div class="author__avatar"> <img src="images/me_20240506.jpg" class="author__avatar" alt="Canyu Chen"></div>
                <div class="author__content">
                    <h3 class="author__name"> Canyu Chen </h3>
                    <p class="author__bio">Ph.D. student at Illinois Institute of Technology</p>
                </div>
                <div class="author__urls-wrapper"> <button class="btn btn--inverse">Follow</button>
                    <ul class="author__urls social-icons">
                        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Chicago, Illinois</li>
                        <li><a href="mailto:cchen151@hawk.iit.edu"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
                        <li><a href="https://twitter.com/CanyuChen3"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i> Twitter</a></li>
                        <li><a href="https://www.linkedin.com/in/canyu-chen-1b2415100/"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
                        <li><a href="https://github.com/canyuchen"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
                        <li><a href="https://scholar.google.com/citations?hl=en&user=iKfWNy0AAAAJ&view_op=list_works&sortby=pubdate"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
                        <!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=DzVxbNa1d5i0c4qQ0vv5ZYvcMVszZxDHALZxQhrnUuQ&cl=ffffff&w=a"></script> -->
                    </ul>
                </div>
            </div>
        </div>
        <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
            <div class="page__inner-wrap">
                <section class="page__content" itemprop="text">
                    <p>Hi, this is Canyu Chen (陈灿宇), a third-year Computer Science
                        Ph.D. student at <em>Illinois Institute of Technology
                            (IIT)</em>
                        since Fall 2021, advised by <a href="https://www.cs.emory.edu/~kshu5/">Prof. Kai Shu</a>. Before
                        joining IIT, I received my <em>B.S.</em> in Computer Science from the <em>
                            University of Chinese Academy of Sciences (UCAS)
                        </em> in 2020.</p>

                    <p>I focus on <strong>Truthful, Safe and Responsible Large Language Models</strong> with
                        the applications in
                        <strong>Social Computing and Healthcare</strong>. I have started and currently lead the <em><strong><a
                                    href="https://llm-misinformation.github.io/">LLMs Meet
                                    Misinformation</a></strong></em> initiative, aiming to combat misinformation in the age of LLMs. I aim to pursue <strong>Safe and Aligned Artificial General Intelligence</strong> in the long run.
                        I am always happy to chat and discuss potential collaborations, or give talks about my research in related seminars. Feel free to contact me via email (cchen151 AT hawk.iit.edu) or wechat (ID: alexccychen).
                    </p>
                    <h2 id="news">News</h2>
                    <!-- <h3 id="news">News</h3> -->
                    <div class="highlight">
                        <!-- <pre> -->
                        <ul>
                            <li>[08/2024] New survey paper is online <a href="https://arxiv.org/abs/2408.08946">Authorship Attribution in the Era of LLMs: Problems, Methodologies, and Challenges</a>, more details: <a href="https://llm-authorship.github.io/">[project website]</a> <a href="https://github.com/llm-authorship/survey">[Paper list on GitHub]</a>
                            <li>[08/2024] I will give a <b><font color="red">Research Spotlight oral presentation</font></b> titled "Combating Misinformation in the Age of LLMs" at <i><a href="https://rdi.berkeley.edu/events/decentralizationaisummit24" style="text-decoration:none"><font color="#494e52">The 2024 Summit on Responsible Decentralized Intelligence —— Future of Decentralization and AI</font></a></i>, hosted by <a href="https://rdi.berkeley.edu/" style="text-decoration:none"><font color="#494e52">The Berkeley Center for Responsible, Decentralized Intelligence (Berkeley RDI)</font></a> 
                                <a href="https://drive.google.com/file/d/12SbWsh6N_-a2-y-ZljDMCoMRq8z1yNO4/view?usp=sharing">[Slides]</a>
                                <a href="https://youtu.be/4Tt4GYQ-ksk?t=5424">[YouTube]</a>
                            <li>[07/2024] New preprint is online <a href="https://arxiv.org/abs/2407.20224">Can Editing LLMs Inject Harm?</a>, more details: <a href="https://llm-editing.github.io/">[project website]</a> <a href="https://github.com/llm-editing/editing-attack">[Code, Results, Dataset on GitHub]</a>
                            <li>[07/2024] New preprint is online <a href="https://arxiv.org/abs/2407.04842">MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?</a>, more details: <a href="https://mj-bench.github.io/">[project website]</a> <a href="https://github.com/MJ-Bench/MJ-Bench">[Code on GitHub]</a> <a href="https://huggingface.co/MJ-Bench">[Models, Datasets, and Leaderboard on huggingface]</a> 
                            <li>[06/2024] Our paper <a href="https://arxiv.org/abs/2306.05949">Evaluating the Social Impact of Generative AI Systems in Systems and Society</a> is forthcoming in <strong><em><font color="#494e52">Oxford Handbook on the Foundations and Regulation of Generative AI</font></em></strong> (Oxford University Press).</li>
                            <li>[06/2024] Invited by <a href="https://yuxuanliang.com/">Prof. Yuxuan Liang</a> to talk at <a href="https://mp.weixin.qq.com/s/GuP99sX5thRlCFTZIeYevw">Swarma Club</a>: "Can Large Language Model Agents Simulate Human Trust Behaviors?". <a href="https://drive.google.com/file/d/1mJp-J0aT__Pr7sQ4mmTXtQRPV9CZTlh2/view?usp=sharing">[Slides]</a></li>
                            <li>[05/2024] Invited to talk at <a href="https://ds.ibs.re.kr/">KAIST/IBS Data Science Group</a>: "Combating Misinformation in the Age of Large Language Models (LLMs)". <a href="https://drive.google.com/file/d/1L65fdYwxQIX64ibbzrA8hCbv0EWglGL5/view?usp=sharing">[Slides]</a></li>
                            <li>[04/2024] Our survey paper <a href="https://arxiv.org/abs/2311.05656">Combating Misinformation in the Age of LLMs: Opportunities and Challenges</a> is accepted to <strong><em><font color="#494e52">AI Magazine 2024</font></em></strong>. <a href="http://doi.org/10.1002/aaai.12188">[publication]</a> <a href="https://github.com/llm-misinformation/llm-misinformation-survey">[paper list]</a></li>
                            <li>[03/2024] Deeply honored and humbled to receive the prestigious 🏆 <a href="https://drive.google.com/file/d/1jJTgXN1kEaMI9ifyZ7X88QLOxYPsbYnf/view?usp=sharing" style="text-decoration:none"><strong><font color="red">Sigma Xi Student Research Award 2024</font></strong></a> from Illinois Tech and the local Sigma Xi chapter. Thanks to <b><a href="https://today.iit.edu/recognizing-the-outstanding-work-of-our-illinois-tech-faculty/?_kx=14_wAl5YyxJSN46wDrmfnCwDD9HmAyBMdAF2cxrZQHA.SzrEDJ">Illinois Tech Today</a></b> for the coverage.</li>
                        </ul>
                        <details>
                        <summary><b>Older News</b></summary>
                        <ul>
                            <li>[02/2024] New preprint is online <a href="https://arxiv.org/abs/2402.04559">Can Large Language Model Agents Simulate Human Trust Behaviors?</a>, <a href="http://agent-trust.camel-ai.org/">[project website]</a> Code and results have been released for verification. <a href="https://github.com/camel-ai/agent-trust">[code and results]</a> Demos on HuggingFace: <a href="https://huggingface.co/spaces/camel-ai/agent-trust-Trust-Game-Demo">[Trust Game Demo]</a>
                                <a href="https://huggingface.co/spaces/camel-ai/agent-trust-Repeated-trust-game-Demo">[Repeated Trust Game Demo]</a>.</li>
                            <li>[01/2024] <a href="https://arxiv.org/abs/2309.13788">Can LLM-Generated Misinformation Be Detected?</a> is accepted to <em><b>ICLR 2024</b></em>, <a href="https://llm-misinformation.github.io/">[project website]</a> <a href="https://github.com/llm-misinformation/llm-misinformation/">[dataset and code]</a>.</li>
                            <li>[12/2023] Honored to receive 🏆 <a href="https://sites.google.com/view/icbinb-2023/papers?authuser=0" style="text-decoration:none"><strong><font color="red">Didactic Paper Award</font></a> </strong> (1/35 of all accepted papers) in workshop <em><b><a href="https://sites.google.com/view/icbinb-2023/papers?authuser=0" style="text-decoration:none"><font color="#494e52">ICBINB@NeurIPS 2023</font></a></b></em> for <a href="https://arxiv.org/abs/2309.13788">Can LLM-Generated Misinformation Be Detected?</a>.</li>
                            <li>[10/2023] Start an initiative <em><strong><a href="https://llm-misinformation.github.io/">LLMs Meet Misinformation</a></strong></em> along with a new survey paper <a href="https://arxiv.org/abs/2311.05656">Combating Misinformation in the Age of LLMs: Opportunities and Challenges</a>, <a href="https://llm-misinformation.github.io/">[project website]</a>  and a paper list collecting related papers and resources <a href="https://github.com/llm-misinformation/llm-misinformation-survey">[paper list]</a>.</li>
                            <li>[10/2023] Honored to be covered by Illinois Tech News on the research of Trustworthy AI, <a href="https://www.iit.edu/student-experience/student-and-alumni-stories/breaking-biases">[IIT News]</a>.</li>
                            <li>[09/2023] New preprint is online <a href="https://arxiv.org/abs/2309.13788">Can LLM-Generated Misinformation Be Detected?</a>, <a href="https://llm-misinformation.github.io/">[project website]</a>. The dataset and code are released <a href="https://github.com/llm-misinformation/llm-misinformation/">[dataset and code]</a>.</li>
                            <!-- <li>[10/2023] New survey paper <a href="https://llm-misinformation.github.io/static/pdf/Combating%20Misinformation%20in%20the%20Age%20of%20LLMs:%20Opportunities%20and%20Challenges.pdf">Combating Misinformation in the Age of LLMs: Opportunities and Challenges</a>, <a href="https://llm-misinformation.github.io/">[project website]</a></li>
                            <li>[10/2023] Honored to be covered by Illinois Tech News on research of Trustworthy AI, <a href="https://www.iit.edu/student-experience/student-and-alumni-stories/breaking-biases">[IIT News]</a></li>
                            <li>[09/2023] New preprint <a href="https://arxiv.org/abs/2309.13788">Can LLM-Generated Misinformation Be Detected?</a>, <a href="https://llm-misinformation.github.io/">[project website]</a></li> -->
                            <!-- <li>[06/2023] Will attend <a href="https://facctconference.org/2023/">FAccT 2023</a> as a volunteer. Welcome to Chicago and glad to connect!</li>
                            <li>[05/2023] One paper accepted at <a href="https://2023.eacl.org/">EACL 2023</a> and will attend online. Welcome to our poster!</li>
                            <li>[04/2023] Glad to be invited by Prof. <a href="https://lucheng.ml/">Lu Cheng</a> to give a talk on AI Fairness at UIC <a href="https://drive.google.com/file/d/1U9CkO_nOTKDJP1DycKrYW2r0IugXLjqh/view?usp=sharing">[Slides]</a></li>
                            <li>[11/2022] Attend <a href="https://nips.cc/">NeurIPS 2022</a> in person. See you at New Orleans!</li>
                            <li>[08/2022] Attend <a href="https://kdd.org/kdd2022/">KDD 2022</a> in person. Glad to meet old friends and make new friends!</li> -->
                            <!-- </pre> -->

                            <!-- <li>[02/2024] New preprint is online <a href="https://arxiv.org/abs/2402.04559">Can Large Language Model Agents Simulate Human Trust Behaviors?</a>, <a href="http://agent-trust.camel-ai.org/">[project website]</a> Code and results have been released for verification. <a href="https://github.com/camel-ai/agent-trust">[code and results]</a> Demos on HuggingFace: <a href="https://huggingface.co/spaces/camel-ai/agent-trust-Trust-Game-Demo">[Trust Game Demo]</a>
                                <a href="https://huggingface.co/spaces/camel-ai/agent-trust-Repeated-trust-game-Demo">[Repeated Trust Game Demo]</a>.</li>
                            <li>[01/2024] <a href="https://arxiv.org/abs/2309.13788">Can LLM-Generated Misinformation Be Detected?</a> is accepted to <em><b>ICLR 2024</b></em>, <a href="https://llm-misinformation.github.io/">[project website]</a> <a href="https://github.com/llm-misinformation/llm-misinformation/">[dataset and code]</a>.</li>
                            <li>[12/2023] Honored to receive 🏆 <a href="https://sites.google.com/view/icbinb-2023/papers?authuser=0" style="text-decoration:none"><strong><font color="red">Didactic Paper Award</font></a> </strong> (1/35 of all accepted papers) in workshop <em><b><a href="https://sites.google.com/view/icbinb-2023/papers?authuser=0" style="text-decoration:none"><font color="#494e52">ICBINB@NeurIPS 2023</font></a></b></em> for <a href="https://arxiv.org/abs/2309.13788">Can LLM-Generated Misinformation Be Detected?</a>.</li>
                            <li>[10/2023] Start an initiative <em><strong><a href="https://llm-misinformation.github.io/">LLMs Meet Misinformation</a></strong></em> along with a new survey paper <a href="https://arxiv.org/abs/2311.05656">Combating Misinformation in the Age of LLMs: Opportunities and Challenges</a>, <a href="https://llm-misinformation.github.io/">[project website]</a>  and a paper list collecting related papers and resources <a href="https://github.com/llm-misinformation/llm-misinformation-survey">[paper list]</a>.</li>
                            <li>[10/2023] Honored to be covered by Illinois Tech News on the research of Trustworthy AI, <a href="https://www.iit.edu/student-experience/student-and-alumni-stories/breaking-biases">[IIT News]</a>.</li>
                            <li>[09/2023] New preprint is online <a href="https://arxiv.org/abs/2309.13788">Can LLM-Generated Misinformation Be Detected?</a>, <a href="https://llm-misinformation.github.io/">[project website]</a>. The dataset and code are released <a href="https://github.com/llm-misinformation/llm-misinformation/">[dataset and code]</a>.</li> -->
                            <!-- <li>[10/2023] New survey paper <a href="https://llm-misinformation.github.io/static/pdf/Combating%20Misinformation%20in%20the%20Age%20of%20LLMs:%20Opportunities%20and%20Challenges.pdf">Combating Misinformation in the Age of LLMs: Opportunities and Challenges</a>, <a href="https://llm-misinformation.github.io/">[project website]</a></li>-->
                            <!-- <li>[10/2023] Honored to be covered by Illinois Tech News on research of Trustworthy AI, <a href="https://www.iit.edu/student-experience/student-and-alumni-stories/breaking-biases">[IIT News]</a></li>
                            <li>[09/2023] New preprint <a href="https://arxiv.org/abs/2309.13788">Can LLM-Generated Misinformation Be Detected?</a>, <a href="https://llm-misinformation.github.io/">[project website]</a></li>  -->
                            <li>[06/2023] Will attend <a href="https://facctconference.org/2023/">FAccT 2023</a> as a volunteer. Welcome to Chicago and glad to connect!</li> 
                            <li>[05/2023] One paper accepted at <a href="https://2023.eacl.org/">EACL 2023</a> and will attend online. Welcome to our poster!</li>
                            <li>[04/2023] Glad to be invited by Prof. <a href="https://lucheng.ml/">Lu Cheng</a> to give a talk on AI Fairness at UIC <a href="https://drive.google.com/file/d/1U9CkO_nOTKDJP1DycKrYW2r0IugXLjqh/view?usp=sharing">[Slides]</a></li>
                            <li>[11/2022] Attend <a href="https://nips.cc/">NeurIPS 2022</a> in person. See you at New Orleans!</li>
                            <li>[08/2022] Attend <a href="https://kdd.org/kdd2022/">KDD 2022</a> in person. Glad to meet old friends and make new friends!</li> 
                            <!-- </pre> -->
                        </ul>
                        </details>
                    </div>
                    <!-- <h2 id="news">News</h2>
                    <ul>
                        <li>[01/2024] <a href="https://arxiv.org/abs/2309.13788">Can LLM-Generated Misinformation Be Detected?</a> is accepted to <em><b>ICLR 2024</b></em>, <a href="https://llm-misinformation.github.io/">[project website]</a> <a href="https://github.com/llm-misinformation/llm-misinformation/">[dataset and
                            code]</a>.</li>
                        <li>[12/2023] Honored to receive 🏆 <a href="https://sites.google.com/view/icbinb-2023/papers?authuser=0" style="text-decoration:none"><strong><font color="red">Didactic Paper Award</font></a> 
                        </strong> (1/35 of all accepted papers) in workshop <em><b>ICBINB@NeurIPS 2023</b></em> for <a href="https://arxiv.org/abs/2309.13788">Can LLM-Generated Misinformation Be Detected?</a>.</li>
                        <li>[10/2023] Start an initiative <em><strong><a
                            href="https://llm-misinformation.github.io/">LLMs Meet
                            Misinformation</a></strong></em> along with a new survey paper <a href="https://arxiv.org/abs/2311.05656">Combating Misinformation in the Age of LLMs: Opportunities and Challenges</a>, <a href="https://llm-misinformation.github.io/">[project website]</a>  and a paper list collecting related papers and resources <a href="https://github.com/llm-misinformation/llm-misinformation-survey">[paper
                                list]</a>.</li>
                        <li>[10/2023] Honored to be covered by Illinois Tech News on the research of Trustworthy AI, <a href="https://www.iit.edu/student-experience/student-and-alumni-stories/breaking-biases">[IIT News]</a>.</li>
                        <li>[09/2023] New preprint is online <a href="https://arxiv.org/abs/2309.13788">Can LLM-Generated Misinformation Be Detected?</a>, <a href="https://llm-misinformation.github.io/">[project website]</a>. The dataset and code are released <a href="https://github.com/llm-misinformation/llm-misinformation/">[dataset and
                            code]</a>.</li>
                    </ul> -->
                    <h2 id="publications">Publications</h2>
                    <h3 id="2023">2024</h3>
                    <ul>

                        <li>
                            <p><strong><a href="https://arxiv.org/abs/2407.20224" style="text-decoration:none"><font color="#494e52">Can Editing LLMs Inject Harm?</font></a></strong>
                                <br /><b>Canyu Chen</b>*, <a href="https://baixianghuang.github.io/">Baixiang Huang</a>*, <a href="https://scholar.google.com/citations?user=MD61m08AAAAJ&hl=en">Zekun Li</a>, <a href="https://billchan226.github.io/">Zhaorun Chen</a>, <a href="https://scholar.google.com/citations?user=qALDmfcAAAAJ&hl=en">Shiyang Lai</a>, <a href="https://xiongxiaoxu.github.io/">Xiongxiao Xu</a>, <a href="https://jasonforjoy.github.io/">Jia-Chen Gu</a>, <a href="https://jindonggu.github.io/">Jindong Gu</a>, <a href="https://www.huaxiuyao.io/">Huaxiu Yao</a>, <a href="https://xiaocw11.github.io/">Chaowei Xiao</a>, <a href="https://sites.cs.ucsb.edu/~xyan/">Xifeng Yan</a>, <a href="https://sites.cs.ucsb.edu/~william/">William Yang Wang</a>, <a href="https://www.robots.ox.ac.uk/~phst/">Philip Torr</a>, <a href="https://dawnsong.io/">Dawn Song</a>, <a href="https://www.cs.emory.edu/~kshu5/">Kai Shu</a> (*equal contributions)
                                <br /> Presented in workshop  <em><strong><a href="https://icml-tifa.github.io/" style="text-decoration:none"><font color="#494e52">TiFA@ICML 2024</font></a>, <font color="red">Lightning Talk</font></strong></em> and <em><strong><a href="https://icml-nextgenaisafety.github.io/" style="text-decoration:none"><font color="#494e52">NextGenAISafety@ICML 2024</font></a></strong></em>.
                                <br /> <a href="https://arxiv.org/abs/2407.20224">[arXiv]</a>
                                <a href="https://llm-editing.github.io/">[project website]</a>
                                <a href="https://drive.google.com/file/d/1pUkoDYDxeWl4nhCy74jaDIhxBW7_Sy4g/view?usp=sharing">[poster]</a>
                                <a href="https://github.com/llm-editing/editing-attack">[Code, Results, and Dataset]</a>
                                <a href="https://youtu.be/4Tt4GYQ-ksk?t=5424">[YouTube]</a>
                                <!-- <a href="https://huggingface.co/MJ-Bench">[Models, Datasets, and Leaderboard on huggingface]</a> -->
                                <!-- <br /> <strong>
                                    <font color="red">🏆 Award:</font> <a href="https://huggingface.co/papers?date=2024-07-09" style="text-decoration:none"><font color="red">Top #1 Paper of the day</font></a>
                                </strong> at <em><b><a href="https://huggingface.co/papers?date=2024-07-09" style="text-decoration:none"><font color="#494e52">HuggingFace AK Daily Papers</font></a></b>.</em> -->
                                <br /> <strong>
                                    <font color="red">🏆 Award:</font> <a href="https://rdi.berkeley.edu/events/decentralizationaisummit24" style="text-decoration:none"><font color="red">Research Spotlight</font></a>
                                </strong> in <i><a href="https://rdi.berkeley.edu/events/decentralizationaisummit24" style="text-decoration:none"><font color="#494e52">The 2024 Summit on Responsible Decentralized Intelligence —— Future of Decentralization and AI</font></a></i>, 
                                hosted by <a href="https://rdi.berkeley.edu/" style="text-decoration:none"><font color="#494e52">The Berkeley Center for Responsible, Decentralized Intelligence (Berkeley RDI)</font></a>
                                <br /> <strong>
                                    <font color="#3b5af2">Invited Talks</font>
                                </strong> : 
                                <a href="https://drive.google.com/file/d/1MHyJO777OlMVuEtNXe7JyXWP9COxUsx9/view?usp=sharing">[Berkeley Decentralization & AI Summit Research Spotlight Talk]</a>
                            </p>
                        </li>

                        <li>
                            <p><strong><a href="https://arxiv.org/abs/2311.05656" style="text-decoration:none"><font color="#494e52">Combating Misinformation in the Age of LLMs: Opportunities and
                                    Challenges</font></a></strong>
                                <br /><strong>Canyu Chen</strong>,
                                <a href="https://www.cs.emory.edu/~kshu5/">Kai Shu</a>.
                                <br />
                                Published in <strong><em>
                                    <font color="#494e52">AI Magazine 2024 (Volume 45, Issue 3, Fall 2024)</font>, <a href="http://doi.org/10.1002/aaai.12188" style="text-decoration:none"><font color="red">Highlight Article</font></a></em></strong>.
                                <br /> 
                                <a href="http://doi.org/10.1002/aaai.12188">[publication]</a>
                                <a href="https://arxiv.org/abs/2311.05656">[arXiv]</a>
                                <a href="https://llm-misinformation.github.io/">[project website]</a>
                                <a href="https://drive.google.com/file/d/1L65fdYwxQIX64ibbzrA8hCbv0EWglGL5/view?usp=sharing">[Slides]</a>
                                <a href="https://github.com/llm-misinformation/llm-misinformation-survey">[paper list]</a>
                                <a href="https://youtu.be/4Tt4GYQ-ksk?t=5424">[YouTube]</a>
                                <!-- <span data-badge-type="2" class='altmetric-embed' data-link-target="_blank" data-arxiv-id='2311.05656'></span> -->
                                 <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=iKfWNy0AAAAJ&sortby=pubdate&citation_for_view=iKfWNy0AAAAJ:4TOpqqG69KYC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> 
                                    <img src="https://img.shields.io/badge/scholar-52-4285F4?logo=googlescholar&amp;labelColor=beige" alt="Google Scholar citations"> </a>
                                    <!-- <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=iKfWNy0AAAAJ&sortby=pubdate&citation_for_view=iKfWNy0AAAAJ:4TOpqqG69KYC"
                                        aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank">
                                        <img id="scholar-badge" src="" alt="Google Scholar citations">
                                    </a> -->
                                    <!-- <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=iKfWNy0AAAAJ&sortby=pubdate&citation_for_view=iKfWNy0AAAAJ:4TOpqqG69KYC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> 
                                        <img id="scholar-badge" src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&labelColor=beige" alt="Google Scholar citations"> 
                                    </a> -->
                                <!-- <br /> <strong>
                                    <font color="red">🏆 Award:</font> <a href="http://doi.org/10.1002/aaai.12188" style="text-decoration:none"><font color="red">Highlight Article</font></a></strong>  of <em> <strong>AI Magazine (Volume 45, Issue 3, Fall 2024)</strong> </em> -->
                                <br /> <strong>
                                    <font color="#3b5af2">Media Coverage</font>
                                </strong> : 
                                <a href="https://www.marktechpost.com/2024/01/27/this-ai-report-from-the-illinois-institute-of-technology-presents-opportunities-and-challenges-of-combating-misinformation-with-llms/">[Marktechpost AI Research News]</a> 
                                <a href="https://www.reddit.com/r/machinelearningnews/comments/1acbgti/this_ai_report_from_the_illinois_institute_of/">[Reddit r/machinelearningnews]</a>
                                <a href="https://www.analyticsvidhya.com/blog/2024/02/are-llms-outsmarting-humans-in-crafting-persuasive-misinformation/">[Analytics Vidhya Blog]</a>.
                                <br /> <strong>
                                    <font color="#3b5af2">Invited Talks</font>
                                </strong> : 
                                <a href="https://drive.google.com/file/d/1MHyJO777OlMVuEtNXe7JyXWP9COxUsx9/view?usp=sharing">[Berkeley Decentralization & AI Summit Research Spotlight Talk]</a>
                                <a href="https://drive.google.com/file/d/1L65fdYwxQIX64ibbzrA8hCbv0EWglGL5/view?usp=sharing">[KAIST/IBS Data Science Group]</a>
                                <a href="https://mp.weixin.qq.com/s/nZgzQzvaVda1fLTXn1YAzQ">[Psych Methods]</a>.
                                <br />
                            </p>
                        </li>
                        <li>
                            <p><strong><a href="https://arxiv.org/abs/2402.04559" style="text-decoration:none"><font color="#494e52">Can Large Language Model Agents Simulate Human Trust Behaviors?</font></a></strong>
                                <br /> <a href="https://yitianlian.github.io/">Chengxing Xie</a>*, <strong>Canyu Chen</strong>*, <a href="https://feiran.io/">Feiran Jia</a>, <a href="https://ziyu-deep.github.io/">Ziyu Ye</a>, <a href="https://www.cs.emory.edu/~kshu5/">Kai Shu</a>, <a href="https://www.adelbibi.com/">Adel Bibi</a>, <a href="https://acbull.github.io/">Ziniu Hu</a>, <a href="https://eng.ox.ac.uk/people/philip-torr/">Philip Torr</a>, <a href="https://www.bernardghanem.com/">Bernard Ghanem</a>, <a href="https://ghli.org/">Guohao Li</a>. (*equal contributions)
                                <br /> Presented in workshop <em> <strong><a href="https://agiworkshop.github.io/" style="text-decoration:none"><font color="#494e52">AGI@ICLR 2024</font></a></strong> </em> and <em> <strong><a href="https://sites.google.com/site/nlpandcss/nlp-css-at-naacl-2024" style="text-decoration:none"><font color="#494e52">NLP+CSS@NAACL 2024</font></a></strong> </em>
                                     <br /> <em>Seventeenth Midwest Speech and Language Days Symposium
                                     (<strong>
                                        <a href="https://ai.engin.umich.edu/news/midwest-speech-and-language-days/" style="text-decoration:none"><font color="#494e52">MSLD 2024</font></a></strong>, <b>
                                        <font color="red">Oral</font>
                                    </b>)</em>
                                    <br /> <em>The First Workshop on AI Behavioral Science
                                        (<strong>
                                           <a href="https://ai-behavioral-science.github.io/2024" style="text-decoration:none"><font color="#494e52">AIBS@KDD 2024</font></a></strong>, <b>
                                           <font color="red">Oral</font>
                                       </b>)</em>.
                                <br /> <a href="https://arxiv.org/abs/2402.04559">[arXiv]</a>
                                <a href="http://agent-trust.camel-ai.org/">[project website]</a>
                                <a href="https://drive.google.com/file/d/1mJp-J0aT__Pr7sQ4mmTXtQRPV9CZTlh2/view?usp=sharing">[slides]</a>
                                <a href="https://github.com/camel-ai/agent-trust">[code and results]</a>
                                <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=iKfWNy0AAAAJ&sortby=pubdate&citation_for_view=iKfWNy0AAAAJ:Wp0gIr-vW9MC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> 
                                    <img src="https://img.shields.io/badge/scholar-15-4285F4?logo=googlescholar&amp;labelColor=beige" alt="Google Scholar citations"> </a>
                                <!-- <a href="https://drive.google.com/file/d/1L65fdYwxQIX64ibbzrA8hCbv0EWglGL5/view?usp=sharing">[Slides]</a> -->
                                <!-- <span class='altmetric-embed' data-badge-popover='bottom' data-arxiv-id='2402.04559'></span> -->
                                <!-- <span data-badge-type="2" class='altmetric-embed' data-link-target="_blank" data-arxiv-id='2402.04559'></span> -->
                                <br />Demos on HuggingFace:
                                <a href="https://huggingface.co/spaces/camel-ai/agent-trust-Trust-Game-Demo">[Trust Game Demo]</a>
                                <a href="https://huggingface.co/spaces/camel-ai/agent-trust-Repeated-trust-game-Demo">[Repeated Trust Game Demo]</a>
                                <!-- <a href="">[demo]</a> -->
                                <br /> <strong>
                                    <font color="#3b5af2">Invited Talks</font>
                                </strong> : 
                                <a href="https://mp.weixin.qq.com/s/GuP99sX5thRlCFTZIeYevw">[Swarma Club]</a>

                            </p>
                        </li>
                        
                        <li>
                            <p><strong><a href="https://arxiv.org/abs/2309.13788" style="text-decoration:none"><font color="#494e52">Can LLM-Generated Misinformation Be Detected?</font></a></strong>
                                <br /><strong>Canyu Chen</strong>,
                                <a href="https://www.cs.emory.edu/~kshu5/">Kai Shu</a>.
                                <!-- <br /> arXiv preprint. Sept. 2023. -->
                                <br />Published in <em>Proceedings of The Twelfth International Conference on Learning Representations (<strong>
                                        <font color="#494e52">ICLR 2024</font></strong> )</em>
                                <br /> Also presented in workshop <em> <strong>
                                        <a href="https://regulatableml.github.io/" style="text-decoration:none"><font color="#494e52">RegML@NeurIPS 2023</font></a></strong>, <b>
                                        <font color="red">Oral</font>
                                    </b> and <strong>
                                        <a href="https://sites.google.com/view/icbinb-2023/papers?authuser=0" style="text-decoration:none"><font color="#494e52">ICBINB@NeurIPS 2023</font></a></strong>, <b>
                                        <font color="red">spotlight</font></b></em>, and the symposium <em><b><a href="https://drive.google.com/file/d/1MHyJO777OlMVuEtNXe7JyXWP9COxUsx9/view?usp=sharing" style="text-decoration:none"><font color="#494e52">AGI Leap Summit 2024</font></a></b></em>.
                                    <br /> 
                                    <a href="https://openreview.net/forum?id=ccxD4mtkTU">[publication]</a>
                                    <a href="https://arxiv.org/abs/2309.13788">[arXiv]</a>
                                    <a href="https://llm-misinformation.github.io/">[project website]</a>
                                    <a href="https://github.com/llm-misinformation/llm-misinformation/">[dataset and code]</a>
                                    <a href="https://drive.google.com/file/d/1L65fdYwxQIX64ibbzrA8hCbv0EWglGL5/view?usp=sharing">[Slides]</a>
                                    <a href="https://youtu.be/4Tt4GYQ-ksk?t=5424">[YouTube]</a>
                                    <a href="https://zhuanlan.zhihu.com/p/678425256">[zhihu]</a>
                                    <a href="https://x.com/CanyuChen3/status/1749337997340790955?s=20">[twitter/x.com]</a>
                                    <a href="https://www.linkedin.com/posts/canyu-chen-1b2415100_iclr2024-misinformation-llm-activity-7155088736353972224--5Ng?utm_source=share&utm_medium=member_desktop">[LinkedIn]</a>
                                    <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=iKfWNy0AAAAJ&sortby=pubdate&citation_for_view=iKfWNy0AAAAJ:_kc_bZDykSQC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> 
                                        <img src="https://img.shields.io/badge/scholar-76-4285F4?logo=googlescholar&amp;labelColor=beige" alt="Google Scholar citations"> </a>
                                    <!-- <span class='altmetric-embed' data-badge-popover='bottom' data-arxiv-id='2309.13788'></span> -->
                                    <!-- <span data-badge-type="2" class='altmetric-embed' data-link-target="_blank" data-arxiv-id='2309.13788'></span> -->
    
                                <br /> <strong>
                                    <font color="red">🏆 Award:</font> <a href="https://sites.google.com/view/icbinb-2023/papers?authuser=0" style="text-decoration:none"><font color="red">Didactic Paper Award</font></a>
                                </strong> in the workshop <em><b><a href="https://sites.google.com/view/icbinb-2023/papers?authuser=0" style="text-decoration:none"><font color="#494e52">ICBINB@NeurIPS 2023</font></a></b> (1/35 of all accepted papers).</em>
                                <br /> <strong>
                                    <font color="red">🏆 Award:</font> <a href="https://rdi.berkeley.edu/events/decentralizationaisummit24" style="text-decoration:none"><font color="red">Research Spotlight</font></a>
                                </strong> in <i><a href="https://rdi.berkeley.edu/events/decentralizationaisummit24" style="text-decoration:none"><font color="#494e52">The 2024 Summit on Responsible Decentralized Intelligence —— Future of Decentralization and AI</font></a></i>, 
                                hosted by <a href="https://rdi.berkeley.edu/" style="text-decoration:none"><font color="#494e52">The Berkeley Center for Responsible, Decentralized Intelligence (Berkeley RDI)</font></a>
                                <br /> <strong>
                                    <font color="red">🏆 Award:</font> <a href="https://drive.google.com/file/d/1MHyJO777OlMVuEtNXe7JyXWP9COxUsx9/view?usp=sharing" style="text-decoration:none"><font color="red">Spotlight Research</font></a>
                                </strong> in <em><b><a href="https://drive.google.com/file/d/1MHyJO777OlMVuEtNXe7JyXWP9COxUsx9/view?usp=sharing" style="text-decoration:none"><font color="#494e52">AGI Leap Summit 2024</font></a></b>.</em>
                                <br /> <strong>
                                    <font color="red">🏆 Award:</font> <a href="https://drive.google.com/file/d/1sqfVrGz7lYjQQiMNFyRNAeYINr4fzGCT/view?usp=sharing" style="text-decoration:none"><font color="red">Third Place Award</font></a>
                                </strong> in the <em><b><a href="https://drive.google.com/file/d/1sqfVrGz7lYjQQiMNFyRNAeYINr4fzGCT/view?usp=sharing" style="text-decoration:none"><font color="#494e52">Illinois Tech College of Computing Poster Session 2024 (Ph.D. Group)</font></a></b>.</em>
                                <br /> <strong>
                                    <font color="#3b5af2">Included in the curriculum at:</font>
                                </strong>  <a
                                    href="https://github.com/michellejm/LLMs-fall-23">[The City University of New York]</a>.
                                <br /> <strong>
                                    <font color="#3b5af2">Media Coverage</font>
                                </strong> : 
                                <a href="https://www.theregister.com/2024/01/30/llms_misinformation_human/">[The Register]</a>
                                <a href="https://x.com/llm_sec/status/1775946187184500814">[LLM Security]</a>
                                <a href="https://www.analyticsvidhya.com/blog/2024/02/are-llms-outsmarting-humans-in-crafting-persuasive-misinformation/">[Blog 1]</a>
                                <a href="https://www.linkedin.com/pulse/business-keeping-internet-safe-jooho-yeo-fzpzf/?trackingId=Frm6xc2CSJyyBoHSFGDVUQ%3D%3D">[Blog 2]</a>.
                                <br /> <strong>
                                    <font color="#3b5af2">Invited Talks</font>
                                </strong> : 
                                <a href="https://drive.google.com/file/d/1MHyJO777OlMVuEtNXe7JyXWP9COxUsx9/view?usp=sharing">[Berkeley Decentralization & AI Summit Research Spotlight Talk]</a>
                                <a href="https://drive.google.com/file/d/1MHyJO777OlMVuEtNXe7JyXWP9COxUsx9/view?usp=sharing">[AGI Leap Summit Spotlight Research Talk]</a>
                                <a href="https://mp.weixin.qq.com/s/kxulG-96cJWv_5GyG4-2qw">[Tsinghua AI Time]</a>
                                <a href="https://mp.weixin.qq.com/s/nZgzQzvaVda1fLTXn1YAzQ">[Psych Methods]</a> <a href="https://drive.google.com/file/d/1L65fdYwxQIX64ibbzrA8hCbv0EWglGL5/view?usp=sharing">[KAIST/IBS Data Science Group]</a>.

                            </p>
                        </li>

                        <li>
                            <p><strong><a href="https://arxiv.org/abs/2408.08946" style="text-decoration:none"><font color="#494e52">Authorship Attribution in the Era of LLMs: Problems, Methodologies, and Challenges</font></a></strong>
                                <br /> <a href="https://baixianghuang.github.io/">Baixiang Huang</a>, <strong>Canyu Chen</strong>, <a href="https://www.cs.emory.edu/~kshu5/">Kai Shu</a>.
                                <br /> arXiv preprint. Aug. 2024.
                                <br /> <a href="https://arxiv.org/abs/2408.08946">[arXiv]</a>
                                <a href="https://llm-authorship.github.io/">[project website]</a>
                                <a href="https://github.com/llm-authorship/survey">[Paper list on GitHub]</a>
                                <!-- <span data-badge-type="2" class='altmetric-embed' data-link-target="_blank" data-arxiv-id='2403.08213'></span> -->
                            </p>
                        </li>

                        <li>
                            <p><strong><a href="https://arxiv.org/abs/2403.08213" style="text-decoration:none"><font color="#494e52">Can Large Language Models Identify Authorship?</font></a></strong>
                                <br /> <a href="https://baixianghuang.github.io/">Baixiang Huang</a>, <strong>Canyu Chen</strong>, <a href="https://www.cs.emory.edu/~kshu5/">Kai Shu</a>.
                                <br /> arXiv preprint. Mar. 2024.
                                <br /> <a href="https://arxiv.org/abs/2403.08213">[arXiv]</a>
                                <a href="https://llm-authorship.github.io/">[project website]</a>
                                <a href="https://github.com/baixianghuang/authorship-llm">[code]</a>
                                <!-- <span data-badge-type="2" class='altmetric-embed' data-link-target="_blank" data-arxiv-id='2403.08213'></span> -->
                            </p>
                        </li>

                        <li>
                            <p><strong><a href="https://arxiv.org/abs/2407.04842" style="text-decoration:none"><font color="#494e52">MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?</font></a></strong>
                                <br />Zhaorun Chen, Yichao Du, Zichen Wen, Yiyang Zhou, Chenhang Cui, Zhenzhen Weng, Haoqin Tu, Chaoqi Wang, Zhengwei Tong, Qinglan Huang, <strong>Canyu Chen</strong>, Qinghao Ye, Zhihong Zhu, Yuqing Zhang, Jiawei Zhou, Zhuokai Zhao, Rafael Rafailov, Chelsea Finn, Huaxiu Yao
                                <br /> Presented in workshop  <em><strong><a href="https://icml-fm-wild.github.io/" style="text-decoration:none"><font color="#494e52">FM-Wild@ICML 2024</font></a></strong></em>.
                                <br /> <a href="https://arxiv.org/abs/2407.04842">[arXiv]</a>
                                <a href="https://mj-bench.github.io/">[project website]</a>
                                <a href="https://github.com/MJ-Bench/MJ-Bench">[Code]</a>
                                <a href="https://huggingface.co/MJ-Bench">[Models, Datasets, and Leaderboard on huggingface]</a>
                                <br /> <strong>
                                    <font color="red">🏆 Award:</font> <a href="https://huggingface.co/papers?date=2024-07-09" style="text-decoration:none"><font color="red">Top #1 Paper of the day</font></a>
                                </strong> at <em><b><a href="https://huggingface.co/papers?date=2024-07-09" style="text-decoration:none"><font color="#494e52">HuggingFace AK Daily Papers</font></a></b>.</em>
                            </p>
                        </li>

                        <li>
                            <p><strong><a href="https://arxiv.org/abs/2407.21264" style="text-decoration:none"><font color="#494e52">Model Attribution in Machine-Generated Disinformation: A Domain Generalization Approach with Supervised Contrastive Learning</font></a></strong>
                                <br />Alimohammad Beigi, Zhen Tan, Nivedh Mudiam, <strong>Canyu Chen</strong>,
                                <a href="https://www.cs.emory.edu/~kshu5/">Kai Shu</a>, <a href="https://www.public.asu.edu/~huanliu/">Huan Liu</a>.
                                <br /> Published in The 11th IEEE International Conference on Data Science and Advanced Analytics (<b><i>DSAA 2024</i></b>)
                                <br /> <a href="https://arxiv.org/abs/2407.21264">[arXiv]</a>
                                <!-- <span data-badge-type="2" class='altmetric-embed' data-link-target="_blank" data-arxiv-id='2305.10668'></span> -->
                            </p>
                        </li>

                        <li>
                            <p><strong><a href="https://arxiv.org/abs/2404.14757" style="text-decoration:none"><font color="#494e52">SST: Multi-Scale Hybrid Mamba-Transformer Experts for Long-Short Range Time Series Forecasting</font></a></strong>
                                <br /><a href="https://xiongxiaoxu.github.io/">Xiongxiao Xu</a>,
                                <strong>Canyu Chen</strong>,
                                <a href="https://yueqingliang1.github.io/">Yueqing Liang</a>,
                                <a href="https://baixianghuang.github.io/">Baixiang Huang</a>,
                                <a href="https://baithebest.github.io/">Guangji Bai</a>,
                                <a href="https://cs.emory.edu/~lzhao41/">Liang Zhao</a>,
                                <a href="https://www.cs.emory.edu/~kshu5/">Kai Shu</a>.
                                <br /> arXiv preprint. Aug. 2024.
                                <br /> <a href="https://arxiv.org/abs/2404.14757">[arXiv]</a>
                                <!-- <span data-badge-type="2" class='altmetric-embed' data-link-target="_blank" data-arxiv-id='2305.10668'></span> -->
                            </p>
                        </li>

                        <li>
                            <p><strong><a href="https://arxiv.org/abs/2305.10668" style="text-decoration:none"><font color="#494e52">MetaGAD: Learning to Meta Transfer for Few-shot Graph Anomaly Detection.</font></a></strong>
                                <br /><a href="https://xiongxiaoxu.github.io/">Xiongxiao Xu</a>, <a
                                    href="https://kaize0409.github.io/">Kaize Ding</a>, <strong>Canyu Chen</strong>,
                                <a href="https://www.cs.emory.edu/~kshu5/">Kai Shu</a>.
                                <br /> Published in The 11th IEEE International Conference on Data Science and Advanced Analytics (<b><i>DSAA 2024</i></b>)
                                <br /> <a href="https://arxiv.org/abs/2305.10668">[arXiv]</a>
                                <!-- <span data-badge-type="2" class='altmetric-embed' data-link-target="_blank" data-arxiv-id='2305.10668'></span> -->
                            </p>
                        </li>

                        <li>
                            <p><strong><a href="https://arxiv.org/abs/2404.12241" style="text-decoration:none"><font color="#494e52">Introducing v0.5 of the AI Safety Benchmark from MLCommons</font></a></strong>
                                <br /> MLCommons AI Safety Working Group
                                <br /> arXiv preprint. Apr. 2024.
                                <br /> <a href="https://arxiv.org/abs/2404.12241">[arXiv]</a>
                                <a href="https://mlcommons.org/2024/04/mlc-aisafety-v0-5-poc/">[official blog]</a>
                                <!-- <span data-badge-type="2" class='altmetric-embed' data-link-target="_blank" data-arxiv-id='2404.12241'></span> -->
                                <br /> <strong>
                                    <font color="#3b5af2">Media Coverage</font>
                                </strong> : 
                                <a href="https://spectrum.ieee.org/ai-safety-benchmark">[IEEE Spectrum]</a>
                                <a href="https://huggingface.co/papers?date=2024-04-19">[AK Daily Papers]</a>
                                <a href="https://www.marktechpost.com/2024/04/20/this-ai-paper-from-mlcommons-ai-safety-working-group-introduces-v0-5-of-the-groundbreaking-ai-safety-benchmark/">[Marktechpost]</a>
                                <a href="https://aibusiness.com/responsible-ai/ai-safety-benchmark-for-evaluating-language-model-risks-unveiled">[AI Business]</a>
                                <a href="https://www.enterpriseai.news/2024/04/18/stanford-hai-ai-index-report-responsible-ai/">[EnterpriseAI News]</a>
                                <a href="https://www.hpcwire.com/2024/04/16/mlcommons-launches-new-ai-safety-benchmark-initiative/">[HPCwire]</a>
                                <a href="https://www.hackster.io/news/mlcommons-releases-latest-mlperf-tiny-benchmark-results-for-on-device-tinyml-3f820ae12aae">[Hackster.io]</a>
                                <a href="https://elblog.pl/2024/04/16/revolutionizing-ai-safety-the-new-mlcommons-benchmark/">[ELBLOG.PL]</a>
                                <a href="https://siliconangle.com/2024/04/16/mlcommons-announces-first-benchmark-assessing-ai-safety/">[SiliconANGLE]</a>
                                <a href="https://goatstack.ai/topics/introducing-v0-5-of-the-ai-safety-benchmark-from-mlcommons-vhqogz">[GoatStack.ai]</a>.
                            </p>
                        </li>

                        <li>
                            <p><strong><a href="https://arxiv.org/abs/2306.05949" style="text-decoration:none"><font color="#494e52">Evaluating the Social Impact of Generative AI Systems in Systems and Society</font></a></strong>
                                <br />Irene Solaiman, Zeerak Talat, William Agnew, Lama Ahmad, Dylan Baker, Su Lin Blodgett, <strong>Canyu Chen</strong>, Hal Daumé III, Jesse Dodge, Isabella Duan, Ellie Evans, Felix Friedrich, Avijit Ghosh, Usman Gohar, Sara Hooker, Yacine Jernite, Ria Kalluri, Alberto Lusoli, Alina Leidinger, Michelle Lin, Xiuzhu Lin, Sasha Luccioni, Jennifer Mickel, Margaret Mitchell, Jessica Newman, Anaelia Ovalle, Marie-Therese Png, Shubham Singh, Andrew Strait, Lukas Struppek, Arjun Subramonian
                                <br />Forthcoming in <strong><em><font color="#494e52">Oxford Handbook on the Foundations and Regulation of Generative AI</font></em></strong>. Oxford University Press. Jun. 2024.
                                <br /> <a href="https://arxiv.org/abs/2306.05949">[arXiv]</a>
                                <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=iKfWNy0AAAAJ&sortby=pubdate&citation_for_view=iKfWNy0AAAAJ:7PzlFSSx8tAC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> 
                                    <img src="https://img.shields.io/badge/scholar-62-4285F4?logo=googlescholar&amp;labelColor=beige" alt="Google Scholar citations"> </a>
                                <!-- <a href="https://mlcommons.org/2024/04/mlc-aisafety-v0-5-poc/">[official blog]</a> -->
                                <!-- <span data-badge-type="2" class='altmetric-embed' data-link-target="_blank" data-arxiv-id='2404.12241'></span> -->
                                <!-- <br /> <strong>
                                    <font color="#3b5af2">Media Coverage</font>
                                </strong> : 
                                <a href="https://spectrum.ieee.org/ai-safety-benchmark">[IEEE Spectrum]</a>
                                <a href="https://huggingface.co/papers?date=2024-04-19">[AK Daily Papers]</a>
                                <a href="https://www.marktechpost.com/2024/04/20/this-ai-paper-from-mlcommons-ai-safety-working-group-introduces-v0-5-of-the-groundbreaking-ai-safety-benchmark/">[Marktechpost]</a>
                                <a href="https://aibusiness.com/responsible-ai/ai-safety-benchmark-for-evaluating-language-model-risks-unveiled">[AI Business]</a>
                                <a href="https://www.enterpriseai.news/2024/04/18/stanford-hai-ai-index-report-responsible-ai/">[EnterpriseAI News]</a>
                                <a href="https://www.hpcwire.com/2024/04/16/mlcommons-launches-new-ai-safety-benchmark-initiative/">[HPCwire]</a>
                                <a href="https://www.hackster.io/news/mlcommons-releases-latest-mlperf-tiny-benchmark-results-for-on-device-tinyml-3f820ae12aae">[Hackster.io]</a>
                                <a href="https://elblog.pl/2024/04/16/revolutionizing-ai-safety-the-new-mlcommons-benchmark/">[ELBLOG.PL]</a>
                                <a href="https://siliconangle.com/2024/04/16/mlcommons-announces-first-benchmark-assessing-ai-safety/">[SiliconANGLE]</a>
                                <a href="https://goatstack.ai/topics/introducing-v0-5-of-the-ai-safety-benchmark-from-mlcommons-vhqogz">[GoatStack.ai]</a>. -->
                            </p>
                        </li>

                    </ul>
                    <h3 id="2023">2023</h3>
                    <ul>
                        <li>
                            <p><strong><a href="https://arxiv.org/abs/2205.09229" style="text-decoration:none"><font color="#494e52">PromptDA: Label-guided Data Augmentation for Prompt-based Few-shot
                                    Learners.</font></a></strong>
                                <br /><strong>Canyu Chen</strong>,
                                <a href="https://www.cs.emory.edu/~kshu5/">Kai Shu</a>.
                                <br />Published in  <em>Proceedings of the 17th Conference of the European Chapter of the Association
                                    for Computational Linguistics (<strong>
                                        <font color="#494e52">EACL 2023</font></strong>, Main Conference Long Paper)</em>
                                <br />Also presented in workshop <em>  <strong>
                                    <a href="https://neurips2022-enlsp.github.io/" style="text-decoration:none"><font color="#494e52">ENLSP@NeurIPS 2022</font></a></strong>, <b>
                                        <font color="red">Oral (spotlight)</font></b></em>.
                                <br />
                                <a href="https://arxiv.org/abs/2205.09229">[arXiv]</a>
                                <a href="https://github.com/canyuchen/PromptDA">[code]</a>
                                <a href="https://www.youtube.com/watch?v=f6Jh7BIMtWw">[youtube]</a>
                                <a href="https://www.bilibili.com/video/BV1Zm4y127ck">[bilibili]</a>
                                <a
                                    href="https://drive.google.com/file/d/1BqC1ge1n6iNYxpN81qQT9hdjHLuQz_bn/view?usp=sharing">[slides]</a>
                                <a
                                    href="https://drive.google.com/file/d/11z-41wLJLdtmEMin9LnpzD63PhW27vBm/view?usp=sharing">[poster]</a>
                                <!-- <span data-badge-type="2" class='altmetric-embed' data-link-target="_blank" data-arxiv-id='2205.09229'></span> -->
                                <!-- <span data-badge-type="2" class='altmetric-embed' data-link-target="_blank" data-doi='10.18653/v1/2023.eacl-main.41'></span> -->
                                <!-- <a href="https://drive.google.com/file/d/1kh5kpSQ1I-06cYagcAr_wjYljMfZsqM_/view?usp=share_link">[workshop version]</a> -->
                            </p>
                        </li>
                        <li>
                            <p><strong><a href="https://arxiv.org/abs/2206.03656" style="text-decoration:none"><font color="#494e52">Fair Classification via Domain Adaptation: A Dual Adversarial Learning
                                    Approach.</font></a></strong><br />
                                <a href="https://yueqingliang1.github.io/">Yueqing Liang</a>, <strong>Canyu
                                    Chen</strong>, <a href="https://www.linkedin.com/in/tian-tian-3b0a9bb0/">Tian
                                    Tian</a>, <a href="https://www.cs.emory.edu/~kshu5/">Kai Shu</a>.
                                <br />Published in  <strong><em>
                                        <font color="#494e52">Frontiers in Big Data 2023</font>
                                    </em></strong>.<br />
                                <a href="https://www.frontiersin.org/articles/10.3389/fdata.2022.1049565/full?&utm_source=Email_to_authors_&utm_medium=Email&utm_content=T1_11.5e1_author&utm_campaign=Email_publication&field=&journalName=Frontiers_in_Big_Data&id=1049565">[publication]</a>
                                <a href="https://arxiv.org/abs/2206.03656">[arXiv]</a>
                                <!-- <span data-badge-type="2" class='altmetric-embed' data-link-target="_blank" data-doi='10.3389/fdata.2022.1049565'></span> -->
                            </p>
                        </li>
                        <li>
                            <p><strong><a href="https://arxiv.org/abs/2302.07363" style="text-decoration:none"><font color="#494e52">Attacking Fake News Detectors via Manipulating News Social Engagement.</font></a></strong>
                                <br /><a href="https://brucehrwang.com/academic/home">Haoran Wang</a>, <a
                                    href="https://brucehrwang.com/academic/home">Yingtong Dou</a>, <strong>Canyu
                                    Chen</strong>, <a href="https://lichao-sun.github.io/">Lichao Sun</a>, Philip S. Yu,
                                <a href="https://www.cs.emory.edu/~kshu5/">Kai Shu</a>.
                                <br />Published in  <em>Proceedings of The ACM Web Conference 2023 (<strong>
                                        <font color="#494e52">WWW 2023</font>
                                    </strong>).</em>
                                    <br />
                                    <a href="https://arxiv.org/abs/2302.07363">[arXiv]</a>
                                    <a href="https://github.com/hwang219/AttackFakeNews">[code]</a>
                                    <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=iKfWNy0AAAAJ&sortby=pubdate&citation_for_view=iKfWNy0AAAAJ:0EnyYjriUFMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> 
                                        <img src="https://img.shields.io/badge/scholar-24-4285F4?logo=googlescholar&amp;labelColor=beige" alt="Google Scholar citations"> </a>
                                    <!-- <span data-badge-type="2" class='altmetric-embed' data-link-target="_blank" data-arxiv-id='2302.07363'></span> -->
                                <br /> <strong>
                                    <font color="#3b5af2">Media Coverage</font>
                                </strong> : <a
                                    href="https://montrealethics.ai/attacking-fake-news-detectors-via-manipulating-news-social-engagement/">[Montreal
                                    AI Ethics Institute]</a>.</em>
                            </p>
                        </li>
                        <!-- <li>
                            <p><strong>PyGOD: A Python Library for Graph Outlier Detection.</strong><br />
                                Kay Liu, Yingtong Dou, Yue Zhao, Xueying Ding, Xiyang Hu, Ruitong Zhang, Kaize Ding,
                                <strong>Canyu Chen</strong>, Hao Peng, George H. Chen, Zhihao Jia, Philip S.
                                Yu.
                                <br /><em>Journal of Machine Learning Research (<strong>
                                        <font color="#494e52">JMLR 2023</font>
                                    </strong>)</em>.<br /> <a href="https://github.com/pygod-team/pygod">[code]</a>
                                <a href="https://arxiv.org/abs/2204.12095">[arXiv]</a>
                            </p>
                        </li> -->

                    </ul>
                    <h3 id="2022">2022</h3>
                    <ul>


                        <li>
                            <p><strong><a href="https://arxiv.org/abs/2211.05289" style="text-decoration:none"><font color="#494e52">Combating Health Misinformation in Social Media: Characterization, Detection,
                                    Intervention, and Open Issues.</font></a></strong><br />
                                <strong>Canyu Chen*</strong>, <a href="https://brucehrwang.com/academic/home">Haoran
                                    Wang</a>*, <a href="https://understandgreen.com/">Matthew Shapiro</a>, <a
                                    href="https://vivo.weill.cornell.edu/display/cwid-yux4008">Yunyu Xiao</a>, <a
                                    href="https://wcm-wanglab.github.io/">Fei Wang</a>, <a
                                    href="https://www.cs.emory.edu/~kshu5/">Kai Shu</a>. (*equal contributions)
                                <br /> arXiv preprint. Nov. 2022.
                                <br /> <a href="https://arxiv.org/abs/2211.05289">[arXiv]</a>
                                <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=iKfWNy0AAAAJ&sortby=pubdate&citation_for_view=iKfWNy0AAAAJ:roLk4NBRz8UC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> 
                                    <img src="https://img.shields.io/badge/scholar-24-4285F4?logo=googlescholar&amp;labelColor=beige" alt="Google Scholar citations"> </a>
                                <!-- <span data-badge-type="2" class='altmetric-embed' data-link-target="_blank" data-arxiv-id='2211.05289'></span> -->
                            </p>
                        </li>

                        <li>
                            <p><strong><a href="https://arxiv.org/abs/2207.08336" style="text-decoration:none"><font color="#494e52">When Fairness Meets Privacy: Fair Classification with Semi-Private Sensitive
                                    Attributes.</font></a></strong><br />
                                <strong>Canyu Chen</strong>, <a href="https://yueqingliang1.github.io/">Yueqing
                                    Liang</a>, <a href="https://xiongxiaoxu.github.io/">Xiongxiao Xu</a>, <a
                                    href="https://scholar.google.com/citations?user=fZXYI2wAAAAJ&amp;hl=en">Shangyu
                                    Xie</a>, <a href="https://sites.google.com/view/ashishkundu/home/">Ashish Kundu</a>,
                                <a href="https://scholar.google.com/citations?user=9rHwD8wAAAAJ&hl=en">Ali Payani</a>,
                                <a href="https://yhongcs.github.io/index.html">Yuan Hong</a>, <a
                                    href="https://www.cs.emory.edu/~kshu5/">Kai Shu</a>.
                                <br />Presented in  workshop<em>  <strong><a href="https://tsrml2022.github.io/" style="text-decoration:none"><font color="#494e52">TSRML@NeurIPS 2022</font></a>
                                    </strong> and <strong><a href="https://www.afciworkshop.org/afcp2022" style="text-decoration:none"><font color="#494e52">AFCP@NeurIPS 2022</font></a></strong></em>.
                                <br /> <a href="https://arxiv.org/abs/2207.08336">[arXiv]</a>
                                <a
                                    href="https://recorder-v3.slideslive.com/?share=78276&s=d4483c12-ab9f-42ec-953d-da838d9cd163">[Video]</a>
                                <a
                                    href="https://drive.google.com/file/d/1i3tYxj0DWVnBFDGxV0iP2H_-plb7lVD-/view?usp=share_link">[Slides]</a>
                                <a
                                    href="https://drive.google.com/file/d/1Wbi3SDY4cq5Kb1XYS_oWDGZPWMeH2zdQ/view?usp=share_link">[Poster]</a>
                                    <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=iKfWNy0AAAAJ&sortby=pubdate&citation_for_view=iKfWNy0AAAAJ:9ZlFYXVOiuMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> 
                                        <img src="https://img.shields.io/badge/scholar-12-4285F4?logo=googlescholar&amp;labelColor=beige" alt="Google Scholar citations"> </a>
                                <!-- <span data-badge-type="2" class='altmetric-embed' data-link-target="_blank" data-arxiv-id='2207.08336'></span> -->
                                <!-- <a href="https://drive.google.com/file/d/1Kuv07fWHQ5jgAqL60CXIKXds0f3bTV0T/view?usp=share_link">[TSRML version]</a>  -->
                                <!-- <a href="https://drive.google.com/file/d/1XGJA5GR-q07d3bsQWZJV7uVpqAa_86mM/view?usp=share_link">[AFCP version]</a></p> -->
                                <br /> <strong>
                                    <font color="#3b5af2">Media Coverage</font>
                                </strong> : <a
                                    href="https://www.iit.edu/student-experience/student-and-alumni-stories/breaking-biases">[Illinois
                                    Tech News]</a>.</em><br />
                        </li>


                        <li>
                            <p><strong><a href="https://www.mdpi.com/1999-4893/15/9/299" style="text-decoration:none"><font color="#494e52">Artificial Intelligence Algorithms for Treatment of Diabetes.</font></a></strong><br /> <a
                                    href="https://scholar.google.com/citations?user=F3OZmPoAAAAJ&amp;hl=en">Mudassir M.
                                    Rashid</a>, <a
                                    href="https://scholar.google.com/citations?user=njl6K6VfGlAC&amp;hl=en">Mohammad
                                    Reza Askari</a>, <strong>Canyu Chen</strong>, <a
                                    href="https://yueqingliang1.github.io/">Yueqing Liang</a>, <a
                                    href="https://www.cs.emory.edu/~kshu5/">Kai Shu</a>, <a
                                    href="https://sites.google.com/iit.edu/ali-cinar">Ali Cinar</a>.<br />
                                    Published in <strong><em>
                                        <font color="#494e52">Algorithms 2022</font>
                                    </em></strong>.<br /> <a href="https://www.mdpi.com/1999-4893/15/9/299">[Paper]</a>
                                <!-- <span data-badge-type="2" class='altmetric-embed' data-link-target="_blank" data-doi='10.3390/a15090299'></span> -->
                            </p>
                        </li>

                        <li>
                            <p><strong><a href="https://arxiv.org/abs/2206.10071" style="text-decoration:none"><font color="#494e52">BOND: Benchmarking Unsupervised Outlier Node Detection on Static Attributed
                                    Graphs.</font></a></strong><br />
                                Kay Liu, Yingtong Dou, Yue Zhao, Xueying Ding, Xiyang Hu, Ruitong Zhang, Kaize Ding,
                                <strong>Canyu Chen</strong>, Hao Peng, Kai Shu, Lichao Sun, Jundong Li, George H. Chen,
                                Zhihao Jia, Philip S. Yu.
                                <br />Published in  <em>Proceedings of the 36th Conference on Neural Information Processing Systems (<strong>
                                        <font color="#494e52">NeurIPS 2022</font>
                                    </strong>), Datasets and Benchmarks Track</em>.
                                <br /> 
                                <a href="https://arxiv.org/abs/2206.10071">[arXiv]</a>
                                <a href="https://github.com/pygod-team/pygod">[code]</a>
                                <a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=iKfWNy0AAAAJ&sortby=pubdate&citation_for_view=iKfWNy0AAAAJ:LkGwnXOMwfcC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> 
                                    <img src="https://img.shields.io/badge/scholar-77-4285F4?logo=googlescholar&amp;labelColor=beige" alt="Google Scholar citations"> </a>
                                <!-- <span data-badge-type="2" class='altmetric-embed' data-link-target="_blank" data-arxiv-id='2206.10071'></span> -->

                            </p>
                        </li>

                    </ul>
                    <h2 id="talks">Invited Talks</h2>
                    <ul>
                        <li>
                            [08/06/2024] <strong>"Combating Misinformation in the Age of LLMs"</strong> in <i><a href="https://rdi.berkeley.edu/events/decentralizationaisummit24" style="text-decoration:none"><font color="#494e52">The 2024 Summit on Responsible Decentralized Intelligence —— Future of Decentralization and AI</font></a></i>, 
                            hosted by <a href="https://rdi.berkeley.edu/" style="text-decoration:none"><font color="#494e52">The Berkeley Center for Responsible, Decentralized Intelligence (Berkeley RDI)</font></a>
                            <a href="https://drive.google.com/file/d/12SbWsh6N_-a2-y-ZljDMCoMRq8z1yNO4/view?usp=sharing">[Slides]</a>
                            <a href="https://youtu.be/4Tt4GYQ-ksk?t=5424">[YouTube]</a>
                        <br />
                        </li>
                        <li>
                            [06/26/2024] <strong>"Can Large Language Model Agents Simulate Human Trust Behaviors?"</strong> invited by <a href="https://yuxuanliang.com/">Prof. Yuxuan Liang</a> at <a href="https://mp.weixin.qq.com/s/GuP99sX5thRlCFTZIeYevw">Swarma Club</a>
                                <a
                                    href="https://drive.google.com/file/d/1mJp-J0aT__Pr7sQ4mmTXtQRPV9CZTlh2/view?usp=sharing">[Slides]</a>
                                <br />
                        </li>
                        <li>
                            [05/10/2024] <strong>"Combating Misinformation in the Age of Large Language Models (LLMs)"</strong> invited by Wenchao Dong at <a href="https://ds.ibs.re.kr/">KAIST/IBS Data Science Group</a>
                                <a
                                    href="https://drive.google.com/file/d/1L65fdYwxQIX64ibbzrA8hCbv0EWglGL5/view?usp=sharing">[Slides]</a>
                                <br />
                        </li>
                        <li>
                            [04/18/2023] <strong>"Fairness in AI: An Introduction"</strong> invited by <a href="https://lcheng.org/">Prof. Lu Cheng</a> at UIC
                                <a
                                    href="https://drive.google.com/file/d/1U9CkO_nOTKDJP1DycKrYW2r0IugXLjqh/view?usp=sharing">[Slides]</a>
                                <br />
                        </li>
                    </ul>
                    <h2 id="talks">Awards and Fellowship</h2>
                    <ul>
                        <li><strong>Highlight Article</strong> in <em> AI Magazine (Volume 45, Issue 3, Fall 2024) </em>.
                        </li>
                        <li> <a href="https://rdi.berkeley.edu/events/decentralizationaisummit24" style="text-decoration:none"><strong><font color="red">Research Spotlight</font></strong></a> in <i><a href="https://rdi.berkeley.edu/events/decentralizationaisummit24" style="text-decoration:none"><font color="#494e52">The 2024 Summit on Responsible Decentralized Intelligence —— Future of Decentralization and AI</font></a></i>, 
                        hosted by <a href="https://rdi.berkeley.edu/" style="text-decoration:none"><font color="#494e52">The Berkeley Center for Responsible, Decentralized Intelligence (Berkeley RDI)</font></a>
                        </li>
                        <li><strong>Great Review</strong> at ACL Rolling Review 2024 April
                        </li>
                        <li><strong>Travel Award</strong> for Seventeenth Midwest Speech and Language Days (MSLD 2024)
                        </li>
                        <li><a href="https://drive.google.com/file/d/1jJTgXN1kEaMI9ifyZ7X88QLOxYPsbYnf/view?usp=sharing" style="text-decoration:none"><strong><font color="red">Sigma Xi Student Research Award 2024</font></strong></a> from Illinois Tech and the local Sigma Xi chapter. ( An award of $500 is given each year to up to two graduate students at Illinois Tech who have demonstrated significant promise in research and scholarship through their accomplishments. There is only one awardee across the whole university in 2024. )
                        </li>
                        <li><strong>Technical AI Safety Fellowship 2024 Spring</strong> from Harvard AI Safety Student Team.
                        </li>
                        <li><strong>Third Place Award</strong> in the Illinois Tech College of Computing Poster Session 2024 (Ph.D. Group).
                        </li>
                        <li><a href="https://drive.google.com/file/d/1MHyJO777OlMVuEtNXe7JyXWP9COxUsx9/view?usp=sharing" style="text-decoration:none"><strong><font color="red">Spotlight Research</font></strong></a> in the symposium AGI Leap Summit 2024.
                        </li>
                        <li><a href="https://sites.google.com/view/icbinb-2023/papers?authuser=0" style="text-decoration:none"><strong><font color="red">Didactic Paper Award</font></strong></a> (1/35 of all accepted papers) in the workshop ICBINB@NeurIPS 2023.
                        </li>
                        <li><strong>NeurIPS 2023 Volunteer Award</strong>.
                        </li>
                    </ul>

                    <h2 id="talks">Media Coverage</h2>
                    <ul>
                        <li><strong>Illinois Tech Today</strong>: <a href="https://today.iit.edu/recognizing-the-outstanding-work-of-our-illinois-tech-faculty/?_kx=14_wAl5YyxJSN46wDrmfnCwDD9HmAyBMdAF2cxrZQHA.SzrEDJ">"Recognizing the Outstanding Work of Our Illinois Tech Faculty"</a>
                        </li>
                        <li><strong>Marktechpost AI Research News</strong>: <a href="https://www.marktechpost.com/2024/01/27/this-ai-report-from-the-illinois-institute-of-technology-presents-opportunities-and-challenges-of-combating-misinformation-with-llms/">"This AI Report from the Illinois Institute of Technology Presents Opportunities and Challenges of Combating Misinformation with LLMs"</a>
                        </li>
                        <li><strong>The Register</strong>: <a href="https://www.theregister.com/2024/01/30/llms_misinformation_human/">"It's true, LLMs are better than people – at creating convincing misinformation"</a>
                        </li>
                        <li><strong>Illinois Tech News</strong>: <a href="https://www.iit.edu/student-experience/student-and-alumni-stories/breaking-biases">"Breaking Biases"</a>
                        </li>
                        <li><strong>Montreal AI Ethics Institute</strong>: <a href="https://montrealethics.ai/attacking-fake-news-detectors-via-manipulating-news-social-engagement/">"Attacking Fake News Detectors via Manipulating News Social Engagement"</a>
                        </li>
                        <li><strong>IEEE Spectrum</strong>: <a href="https://spectrum.ieee.org/ai-safety-benchmark">"Announcing a Benchmark to Improve AI Safety: MLCommons has made benchmarks for AI performance—now it's time to measure safety"</a>
                        </li>
                    </ul>

                </section>
                <footer class="page__meta"></footer>
            </div>
        </article>
    </div>
    <div class="page__footer">
        <footer></footer>
    </div>
    <script src="assets/js/main.min.js"></script>
    <script> (function (i, s, o, g, r, a, m) { i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () { (i[r].q = i[r].q || []).push(arguments) }, i[r].l = 1 * new Date(); a = s.createElement(o), m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m) })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga'); ga('create', '', 'auto'); ga('send', 'pageview'); </script>


    <script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'></script>

    <!-- Statcounter code for homepage https://canyuchen.com/ on
Google Sites (new) -->
    <script type="text/javascript">
        var sc_project = 12925672;
        var sc_invisible = 1;
        var sc_security = "fc1d0147"; 
    </script>
    <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
    <noscript>
        <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
                    class="statcounter" src="https://c.statcounter.com/12925672/0/fc1d0147/1/" alt="Web Analytics"
                    referrerPolicy="no-referrer-when-downgrade"></a></div>
    </noscript>
    <!-- End of Statcounter Code -->
</body>

</html>